{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49faa325",
   "metadata": {},
   "source": [
    "# Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd57dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fec47b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d87d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('mnist_train.csv')\n",
    "test = pd.read_csv('mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c773c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.iloc[:, 1:]/255\n",
    "y_train = train.iloc[:, 0]\n",
    "X_test = test.iloc[:, 1:]/255\n",
    "y_test = test.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d32744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = StandardScaler().fit_transform(X_train)\n",
    "# pca = PCA()\n",
    "# X_train = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecee89f",
   "metadata": {},
   "source": [
    "### 1. The pixel values in both the training set ranges in [0,255], thus divide them by 255 to normalize them between [0, 1]\n",
    "\n",
    "### 2. Individually train Logistic Regression, LDA and QDA on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd62c1e",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e5b66d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colin/opt/miniconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "lrpred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c302844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9258"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(lrpred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3f23c8",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "007a861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis().fit(X_train, y_train)\n",
    "ldapred = lda.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7db9cd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.873"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ldapred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab9a6f",
   "metadata": {},
   "source": [
    "#### QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "286edc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colin/opt/miniconda3/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5553"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda = QuadraticDiscriminantAnalysis().fit(X_train, y_train)\n",
    "qdapred = qda.predict(X_test)\n",
    "accuracy_score(qdapred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c0f22",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e25caca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8235\n"
     ]
    }
   ],
   "source": [
    "# prepare the cross-validation procedure\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "# create model\n",
    "naive_bayes = MultinomialNB()\n",
    "# evaluate model\n",
    "scores = cross_val_score(naive_bayes, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.4f' % (np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36864895",
   "metadata": {},
   "source": [
    "It seems that logistic regression performed the best all of four algorithems and QDA performed worst. Logistics regression is the simplest model of all, which does not seem to underfit the mnist dataset, while quadratic discriminant analysis performs the worst, probably due to possible overfitting on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455859c1",
   "metadata": {},
   "source": [
    "# Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa362db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "college = pd.read_csv('College.csv')\n",
    "X = college.drop(['Unnamed: 0', 'Apps'], axis = 1)\n",
    "y = college['Apps']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8784a2b",
   "metadata": {},
   "source": [
    "### 1. Split the data set into a training set and a test set, normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d54f9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only categorical variable: private (yes/no), convert it (1/0)\n",
    "X = pd.get_dummies(data=X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0322ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized all columns except categorical feature 'Private_Yes'\n",
    "scaler = StandardScaler()\n",
    "X.loc[:, X.columns != 'Private_Yes'] = scaler.fit_transform(X.loc[:, X.columns != 'Private_Yes'])\n",
    "# check result: mean=zero, variance=1\n",
    "# X.mean(axis=0)\n",
    "# X.var(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdaed5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6d97fb",
   "metadata": {},
   "source": [
    "### 2. Fit linear regression and report test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39815309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 1492443.38\n"
     ]
    }
   ],
   "source": [
    "regr = LinearRegression().fit(X_train, y_train)  # fit linear regression\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, regr.predict(X_test)))  # MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaf4ad8",
   "metadata": {},
   "source": [
    "### 3. Fit a ridge regression model on the training set, with Î» chosen by cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d98bdf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridgecv = RidgeCV()  # leave-one-out cv\n",
    "ridgecv.fit(X_train, y_train)  # fit ridge\n",
    "ridgecv.alpha_  # best alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9979f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 1490697.78\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge(alpha = ridgecv.alpha_)  # using best alpha\n",
    "ridge.fit(X_train, y_train)  # fit ridge\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, ridge.predict(X_test)))  # MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69af77b7",
   "metadata": {},
   "source": [
    "### 4. Fit LASSO regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c6ca955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso alpha:  3.7424119368188964\n"
     ]
    }
   ],
   "source": [
    "lassocv = LassoCV()  # leave-one-out cv\n",
    "lassocv.fit(X_train, y_train)  # fit lasso\n",
    "print('lasso alpha: ', lassocv.alpha_)  # best alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9b58698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 1474198.20\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=lassocv.alpha_)  # using best alpha\n",
    "lasso.fit(X_train, y_train)   # fit lasso\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, lasso.predict(X_test)))  # MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05325504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso nonzero coefficients: [4027.14242701 -780.41613793  869.74492015 -259.00089067  146.69769831\n",
      "  -20.64162858 -277.05730676  164.69712511   17.28003815   21.88049098\n",
      " -160.61002607  -17.41857252   14.06946747   -8.6408197   206.9888744\n",
      "  137.63842589 -609.42279556]\n"
     ]
    }
   ],
   "source": [
    "print('lasso nonzero coefficients:', lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa34aa72",
   "metadata": {},
   "source": [
    "### 5. Comment on results obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34c5291d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8877583168400993"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, regr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f97db81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8878895973260851"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, ridge.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f14753b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8891304757579555"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, lasso.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c27169",
   "metadata": {},
   "source": [
    "It seems that LASSO performed the best, where the 88.91% of the variation in the response variable is explained by the predictors. While ridge regression, 88.79% of the variation is explained by the predictors. Both penalized regression performed similarly. This is probably because even though, ridge did not select variables, it still shrinks the coefficient to be fairly small numbers. Thus, producing similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0daf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
